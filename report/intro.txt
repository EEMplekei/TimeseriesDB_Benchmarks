We have diligently conducted a comprehensive exploration of the prominent database systems listed above, employing a robust benchmark suite renowned as TSBS, denoting the Time Series Benchmark Suite [1]. This suite encapsulates an assortment of Go programs and Shell scripts meticulously engineered to automate the benchmarking process for a specific category of database. It ensures methodological coherence and comparability across a diverse array of scenarios. Leveraging the functionalities inherent in the TSBS tools, we generated three distinct categories of pseudo-random datasets, distinguished by their varying magnitudes of size, aptly labeled as small, medium, and large datasets. Subsequently, we ingested the generated data into both databases and meticulously assessed the relative performance of each database, discerning key metrics such as rows/sec and metrics/sec, which stand as universally acknowledged pivotal performance indicators for data ingestion.

Following the meticulous juxtaposition of data-ingestion rates, we embarked on a nuanced scrutiny of how each database stores files on disk. This analytical endeavor involved the systematic identification of on-disk database files, precise measurement of file sizes for each database, and an exhaustive analysis of diverse compression techniques, especially pertinent when dealing with time-series data. Having scrupulously prepared the databases and measured insert performance, we proceeded to formulate an array of query scenarios, spanning the testing spectrum from multiple succinct queries to one or two protracted queries, each executed on datasets of considerable size.

With the queries systematically generated and meticulously organized within well-structured directories, a meticulous plan was devised for querying the databases and consolidating results for each scenario. To uphold the highest standards of accuracy, a steadfast protocol was adhered to, involving the systematic clearance of the cache at the initiation of every new query or query-set execution. Subsequently, queries were executed under diverse scenarios and infrastructures, encompassing tests on both single and multi-node architectures. The latter aspect, focusing on tests conducted while progressively increasing the data-node count, furnished valuable insights into the scalability and responsiveness of the databases under high-traffic conditions.

To systematically analyze and visually represent the results amassed from multiple benchmarks, we meticulously crafted a series of diagrams using Python scripts. In the conclusive section of this document, we critically scrutinize the outcomes derived from the TSBS benchmark, shedding light on the distinctive advantages that one database holds over the other.