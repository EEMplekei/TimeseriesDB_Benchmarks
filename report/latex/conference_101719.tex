\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Comparative Analysis of InfluxDB and TimescaleDB\\}

\author{
\IEEEauthorblockN{Panagiotis Stefanis}
\IEEEauthorblockA{\textit{Department of Electrical and Computer Engineering} \\
\textit{National Technical University of Athens}\\
Athens, Greece \\
el19096@mail.ntua.gr}\\

\IEEEauthorblockN{Georgios Sotiropoulos}
\IEEEauthorblockA{\textit{Department of Electrical and Computer Engineering} \\
\textit{National Technical University of Athens}\\
Athens, Greece \\
el19848@mail.ntua.gr}\\

\IEEEauthorblockN{Michail Tsilimigkounakis}
\IEEEauthorblockA{\textit{Department of Electrical and Computer Engineering} \\
\textit{National Technical University of Athens}\\
Athens, Greece \\
el19001@mail.ntua.gr}\\
}


\maketitle

\begin{abstract}
In the era characterized by the prevalence of sensors and the Internet of Things, timestamped data has become ubiquitous across diverse large-scale information systems, spanning from financial indices to physical measurements. The imperative for facile and efficient storage and retrieval of time-series data has reached unprecedented levels, prompting numerous renowned data engineering and database enterprises to proffer specialized solutions tailored for timestamped data. This study undertakes a comparative analysis of two prominent database systems within this domain, evaluating their performance, compression techniques, and multi-node scalability. Specifically, the selected database systems under scrutiny are InfluxDB and TimescaleDB, widely acknowledged as preeminent products in terms of performance, maturity, and seamless integration. This paper aims to assist decision-makers, developers, and data engineers in making informed choices based on the specific requirements of their time-sensitive applications.
\end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\section{Introduction}
We have conducted a comprehensive exploration of the prominent database systems listed above, employing a robust benchmark suite renowned as TSBS, denoting the Time Series Benchmark Suite \cite{b1}. This suite encapsulates an assortment of Go programs and Shell scripts engineered to automate the benchmarking process for a specific category of databases. It ensures methodological coherence and comparability across a diverse array of scenarios. Utilizing the functionalities inherent in the TSBS tools, we generated three distinct datasets, distinguished by their varying magnitudes of size, labeled as small, medium, and large datasets. Subsequently, we ingested the generated data into both databases and assessed the relative performance of each database, discerning key metrics such as rows/sec and metrics/sec, which stand as universally acknowledged pivotal performance indicators for data ingestion. 

Afterwards, we took on a study of how each database stores files on disk. This analytical endeavor involved the systematic identification of on-disk database files, precise measurement of file sizes for each database, and a analysis of diverse compression techniques, especially pertinent when dealing with time-series data. Having prepared the databases and measured insert performance, we proceeded to formulate an array of query scenarios, spanning the testing spectrum from one query to multiple protracted queries.

With the queries systematically generated and organized within well-structured directories, a plan was devised for querying the databases and consolidating results for each scenario. To uphold the highest standards of accuracy, a steadfast protocol was adhered to, involving the systematic clearance of the cache at the initiation of every new query or query-set execution. Subsequently, queries were executed under diverse scenarios and infrastructures, encompassing tests on both single and multi-node architectures. The latter aspect, focusing on tests conducted with increased data-node count, furnished valuable insights into the scalability and responsiveness of the databases. 

To systematically analyze and visually represent the results gathered from multiple benchmarks, we generated a series of diagrams using Python scripts that can be found in the GitHub Repository \cite{b2}. In the conclusive section of this document, we critically study the outcomes derived from the TSBS benchmark, shedding light on the distinctive advantages that one database holds over the other.

\section{Single Node Setup}

\subsection{System Setup}

To execute our benchmarking procedure in the single-node mode of Timescale (an extension of Postgres) and Influx database systems, we intend to establish a virtual machine on the Okeanos Infrastructure as a Service (IaaS) cloud provider. 

As a virtual machine, we have instantiated an Ubuntu 22.04.03 LTS instance, utilizing the 5.15.0-91-generic Linux kernel. The virtual environment is configured with an Intel(R) Xeon(R) CPU E5-2650 v3 operating at 2.3GHz, 4GB of RAM, and a 30GB virtual boot drive. The virtual machine is automatically assigned a public IPv6 address, and a domain is provided to facilitate seamless SSH access. Furthermore, a public IPv4 address (83.212.73.15) has been specifically assigned to enable compatibility with services requiring IPv4 connectivity, such as GitHub, which lacks native support for IPv4 and necessitates the implementation of a NAT64 system, which would require extra resources to set up and deploy.

\subsection{InfluxDB Installation}
To commence, we will install Influx version 1.8.10 \cite{b3}. This entails adding Influx's repository to the apt sources list and proceeding with the installation through the apt package manager. Following the installation, we will initiate the influxdb service to ensure that the Database Management System (DBMS) operates in the background on our benchmarking machine.

\subsection{TimescaleDB Installation}
The second timeseries database, Timescale, operates as an extension built upon PostgresSQL. To facilitate the installation of Timescale, it is imperative to first install Postgres \cite{b4}. We will utilize version 14.10 of Postgres for this purpose. As a preliminary step, we need to install specific third-party packages. Following this, we can execute a provided shell script by Postgres to seamlessly handle the installation process.

Upon the successful installation of Postgres on our virtual machine, the subsequent step involves the installation and activation of Timescale. For compatibility reasons, we have opted to use version 2.13.0 \cite{b4}, as it was the latest version available at the time of installation. In a manner similar to the procedure employed for Influx, the inclusion of Timescale's repository in the apt sources list is required, followed by the initiation of the installation process through the apt package manager. 

Upon completion of the installation, we are required to execute a tuning script provided by Timescale. Finally, we utilize Postgres's Command Line Interface (CLI) environment to register the Timescale extension with the Database Management System (DBMS).

\subsection{TSBS Installation}
The benchmarking system designated for our analysis is referred to as TSBS (Timeseries Database Benchmarking System) and is under the maintenance of Timescale. While TSBS is capable of benchmarking numerous timeseries databases, our focus will be on evaluating two specific databases, namely Timescale and Influx. Developed in the Go programming language, TSBS necessitates compilation on our local machine. Therefore, our initial step involves the installation of the Go, a process facilitated through the apt package manager. For the present analysis, we are installing the latest version available, which, at the time of writing, is version 1.18.1.

Following the successful installation of Go, we proceed to clone the official TSBS GitHub repository \cite{b1} and compile it on our system. This involves utilizing Go's built-in installation system to acquire the TSBS repository and subsequently employing the make utility for compilation. Once TSBS is successfully built, it becomes imperative to inform the shell about the paths of TSBS's binary executables. To achieve this, we export the directory containing TSBS's binaries to the PATH environment variable. The scripts housed in this directory will be utilized throughout various stages of this analysis, including data generation, query generation, data insertion, and query execution.

\subsection{Clone Git Repository}
Lastly we have to clone the GitHub repository \cite{b2} of this paper. All of the required scripts  to benchmark the two, now set up, systems are there. Additionally, there exist scripts designed to facilitate the visualization of the generated benchmark data. However, it is essential to note that these scripts are intended for execution on a system equipped with a Graphical User Interface (GUI). This is particularly important as the utilization of the matplotlib, a widely adopted Python plotting library, is contingent upon a GUI environment for graph generation. The performance data, as a result, are saved to the `/performance` directory within the GitHub repository \cite{b2}. Subsequently, every recorded measurement is documented within this designated file.

\section{Data Generation}

In the data generation process, we employed the `tsbs\_generate\_data` script provided by TSBS. Our initial decision aimed at creating three distinct datasets, intending to assess how the two databases would handle variations in dataset sizes. These datasets consist of a small dataset sized at 1GB, a medium dataset sized at 5GB, and a large dataset measuring 15GB.

Concerning the specification of dataset sizes, we determined the appropriate timestamps for the start and end of our data within the `tsbs\_generate\_data` tool. This selection allowed us to generate the three distinct datasets (small - 1GB, medium - 5GB, large - 15GB).

For the selected research use case, we focused on the Internet of Things (IoT). Rigorous consistency was maintained in the application of non-random results across all data and generated queries. This approach ensures the repeatability of IoT data generation, facilitating adjustments in both the database size and dataset dimensions.

This use case is meant to simulate the data load in an IoT environment. This use case simulates data streaming from a set of trucks belonging to a fictional trucking company, simulating diagnostic data and metrics from each truck. Environmental factors are also introduced, such as out-of-order data and batch ingestion (for trucks that are offline for a period of time). It also tracks truck metadata and uses this to tie metrics and diagnostics together as part of the query set.

For those seeking additional technical insights into the data generation process, comprehensive details are available in our script located within our repository at \cite{b2}.


\section{Query Generation}
For the query generation process, we elected to employ all query types provided by TSBS within the context of the IoT use case. Subsequently, a decision was made to assess, for each dataset, the execution of a singular query for each type. Additionally, we conducted evaluations involving the execution of ten consecutive queries of the same type.  This intentional approach was aimed at scrutinizing the indexing and caching performance of the respective databases, while also obtaining a more precise measurement for each query type by calculating the mean value across the executions of ten consecutive queries. The inclusion of ten consecutive queries was considered essential due to the inherent randomness in query execution, which, in a single execution, could introduce inaccuracies into our experiment.

To facilitate these procedures, we adapted and employed the bash script `generate\_queries.sh` to generate queries for the three databases established for both Influx and TimescaleDB. The script, now renamed as `generate\_all\_queries.sh`, resides in the repository \cite{b2}. Its primary function is to parameterize and execute the Go binary `tsbs\_generate\_queries`.

\section{Single Node Insert Performance}

\section{Single Node Size on Disk}

\section{Single Node Average Query Performance}

\section{Single Node Query Performance per Query Type}

\section{Multi Node Setup}


\subsection{System Setup}
To evaluate the scalability of each database, we established a cluster comprising three nodes (virtual machines provided by Okeanos). For each virtual machine, we instantiated an instance of Ubuntu 22.04.03 LTS, running the 5.15.0-91-generic Linux kernel. The virtual environment is equipped with an Intel(R) Xeon(R) CPU E5-2650 v3 operating at 2.3GHz, 4GB of RAM, and a 30GB virtual boot drive. These virtual machines are connected to a virtual private network (VPN) configured through Okeanos. In order to establish internet connectivity for all three machines, Network Address Translation (NAT) was implemented on Node A, which is equipped with the public IPv4 address in this particular instance. This configuration was essential to enable license key verification for the Influx Enterprise data version. However, an elaboration on this aspect will be provided later in this section, specifically within the part of the multi-node Influx installation. TimescaleDB and Influx will be installed on each node, and we will employ the same datasets used in the single-node experiment (small - 1GB, medium - 5GB, and large - 15GB).


\subsection{InfluxDB Installation}
To implement Influx in cluster mode, we had to transition to the Enterprise Edition, the sole version supporting clustering. Influx Enterprise 1.11.3 is a commercial product, but for this project, we utilized the 14-day free trial. The Influx Enterprise Meta node installer was deployed in Node A, while the Influx Enterprise Data node installer was installed in Nodes B and C. Subsequently, we modified the configuration files of Influx on all three nodes to incorporate the license key obtained from Influx Enterprise. Following this, Node A (meta node) and Node B, Node C (data nodes) were added to the cluster, and subsequently inserted data into the retention policy. The same dataset and queries from the single-node experiment were used.


\subsection{TimescaleDB Installation}
For the TimescaleDB cluster, Node A was designated as the access node, while Nodes B and C were assigned as data nodes. We installed TimescaleDB 2.13.1 on all three nodes, mirroring the setup from the single-node experiment. Subsequently, we adjusted the configuration files of PostgreSQL for all three nodes to establish cluster connectivity. Further modifications were made to the configuration of Node A to function as the access node and the configurations of Nodes B and C to operate as data nodes. Following these adjustments, we initiated the creation of the databases Node A, specifying Nodes B and C as the data nodes that would host the data of each database. Subsequently, we populated the hypertables with data using the same queries employed in the single-node experiment.

\subsection{TSBS Installation}


\subsection{Clone Git Repository}


\subsection{Additional Information}
For a detailed step-by-step guide, including the specific commands used in the aforementioned processes, please refer to our GitHub Repository \cite{b2}. There, you will find a comprehensive procedure outlined with commands, providing an in-depth resource for replicating and understanding each phase of the setup for both TimescaleDB and Influx in cluster mode. Feel free to explore the repository for additional insights, documentation, and any updates related to the setup procedures. This resource is designed to facilitate a smooth and transparent replication of the processes described, enhancing your understanding and ensuring accurate implementation.



\section{Multi Node Insert Performance}



\section{Multi Node Size on Disk}



\section{Multi Node Average Query Performance}



\section{Multi Node Query Performance per Query Type}



\section{Summary}


\begin{thebibliography}{00}
\bibitem{b1} https://github.com/timescale/tsbs
\bibitem{b2} https://github.com/EEMplekei/TimeseriesDB\_Benchmarks
\bibitem{b3} https://docs.influxdata.com/influxdb/v1/introduction/install/\#installing-influxdb-oss
\bibitem{b4} https://docs.timescale.com/self-hosted/latest/install/installation-linux
\end{thebibliography}




\end{document}
